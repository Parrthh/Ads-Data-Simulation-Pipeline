# Ads Data Pipeline Simulation

## ğŸ§­ Overview
The **Ads Data Pipeline Simulation** project replicates a real-world advertising analytics pipeline similar to those used by platforms like Google Ads or Meta Ads. It demonstrates the full **data engineering lifecycle** â€” from event generation and streaming ingestion to transformation, storage, and visualization.

This project showcases scalable data engineering concepts and cloud integration using **Python, Kafka, PySpark, Airflow, and AWS**.

---

## ğŸ§± System Architecture

### **High-Level Flow**
```
Python + Faker â†’ Kafka â†’ PySpark â†’ AWS S3 â†’ AWS Glue â†’ AWS Athena â†’ QuickSight
```

### **Components**
| Layer | Tools / Services | Description |
|--------|------------------|--------------|
| **Data Generation** | Python, Faker | Simulate ad events (impressions, clicks, conversions). |
| **Ingestion** | Kafka | Stream data in real-time using producers and consumers. |
| **Processing** | PySpark | Clean, aggregate, and compute metrics (CTR, CPC). |
| **Storage** | AWS S3, Glue, Athena | Store raw and processed data; catalog metadata and enable SQL queries. |
| **Visualization** | AWS QuickSight | Build dashboards showing campaign KPIs. |
| **Orchestration** | Airflow | Schedule and monitor daily ETL workflows. |
| **Infrastructure** | Docker, GitHub Actions | Containerized setup and CI/CD automation. |

---

## âš™ï¸ Setup Instructions

### **1. Clone the Repository**
```bash
git clone https://github.com/<your-username>/ads-data-pipeline.git
cd ads-data-pipeline
```

### **2. Create Virtual Environment & Install Dependencies**
```bash
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### **3. Run Data Simulation Script**
Generate synthetic ad events:
```bash
python scripts/generate_ads_data.py
```
This creates a CSV file in `data/raw/` containing simulated ad event data.

### **4. Start Kafka Services**
Use Docker Compose or a local setup to run Kafka:
```bash
docker-compose up -d
```

### **5. Run the Spark Processing Job**
Execute the ETL job to clean and aggregate data:
```bash
spark-submit scripts/spark_processor.py
```

### **6. Deploy to AWS (Optional)**
- Upload processed data to **S3**
- Run **Glue Crawler** to catalog datasets
- Query via **Athena**
- Visualize in **QuickSight**

---

## ğŸ“Š Metrics Computed
| Metric | Formula | Description |
|---------|----------|--------------|
| **CTR (Click-Through Rate)** | clicks / impressions | Measures ad engagement. |
| **CPC (Cost Per Click)** | cost / clicks | Measures advertising efficiency. |
| **Conversion Rate** | conversions / clicks | Measures ad effectiveness. |

---

## ğŸš€ Future Enhancements
- Real-time Lambda triggers for instant data updates.
- REST API for retrieving campaign-level metrics.
- Integration with Snowflake or Redshift for advanced analytics.
- Monitoring dashboards using Grafana.

---

## ğŸ§© Folder Structure
```
ads-data-pipeline/
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ generate_ads_data.py
â”‚   â”œâ”€â”€ kafka_producer.py
â”‚   â”œâ”€â”€ kafka_consumer.py
â”‚   â”œâ”€â”€ spark_processor.py
â”‚
â”œâ”€â”€ airflow_dags/
â”‚   â””â”€â”€ daily_pipeline_dag.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ PRD.md
â”‚   â””â”€â”€ architecture_diagram.png
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

---

## ğŸ“ˆ Success Metrics
| Objective | Target |
|------------|---------|
| Event Generation | â‰¥ 1 million/day |
| ETL Runtime | â‰¤ 5 minutes |
| Dashboard Latency | < 5 seconds/query |
| Uptime | 99% |

---

## ğŸ§‘â€ğŸ’» Author
**Parth Kharkar**  
Data Engineer & Full Stack Developer  
[LinkedIn](https://www.linkedin.com/in/parrthhkharkar) Â· [GitHub](https://github.com/Parrthh) Â· [Portfolio](https://parthkharkar.lovable.app/?)

---

## ğŸ“„ License
This project is open source and available under the [MIT License](LICENSE).

